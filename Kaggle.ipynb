{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/hsnl-iot/DataMining_2019/VENV/DataMining/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hsnl-iot/DataMining_2019/VENV/DataMining/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hsnl-iot/DataMining_2019/VENV/DataMining/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hsnl-iot/DataMining_2019/VENV/DataMining/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hsnl-iot/DataMining_2019/VENV/DataMining/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hsnl-iot/DataMining_2019/VENV/DataMining/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "100%|██████████| 138999/138999 [00:00<00:00, 241942.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 0    anticipation\n",
      "1         sadness\n",
      "3            fear\n",
      "5             joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " 0    anticipation\n",
      "1         sadness\n",
      "3            fear\n",
      "5             joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "input_shape:  20\n",
      "output_shape:  8\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            6950000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               314368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 72        \n",
      "=================================================================\n",
      "Total params: 7,400,128\n",
      "Trainable params: 450,128\n",
      "Non-trainable params: 6,950,000\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/5\n",
      "1164450/1164450 [==============================] - 842s 723us/step - loss: 1.3942 - acc: 0.4976 - val_loss: 1.3239 - val_acc: 0.5259\n",
      "Epoch 2/5\n",
      "1164450/1164450 [==============================] - 846s 726us/step - loss: 1.3554 - acc: 0.5123 - val_loss: 1.3132 - val_acc: 0.5307\n",
      "Epoch 3/5\n",
      "1164450/1164450 [==============================] - 847s 727us/step - loss: 1.3524 - acc: 0.5137 - val_loss: 1.3099 - val_acc: 0.5297\n",
      "Epoch 4/5\n",
      "1164450/1164450 [==============================] - 844s 724us/step - loss: 1.3486 - acc: 0.5150 - val_loss: 1.3099 - val_acc: 0.5322\n",
      "Epoch 5/5\n",
      "1164450/1164450 [==============================] - 846s 727us/step - loss: 1.3483 - acc: 0.5153 - val_loss: 1.3010 - val_acc: 0.5335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.68      0.16      0.26      8003\n",
      "anticipation       0.66      0.49      0.56     49901\n",
      "     disgust       0.44      0.38      0.41     27449\n",
      "        fear       0.70      0.33      0.45     12844\n",
      "         joy       0.52      0.83      0.64    103127\n",
      "     sadness       0.45      0.47      0.46     38638\n",
      "    surprise       0.72      0.14      0.24      9816\n",
      "       trust       0.61      0.23      0.34     41335\n",
      "\n",
      "    accuracy                           0.53    291113\n",
      "   macro avg       0.60      0.38      0.42    291113\n",
      "weighted avg       0.56      0.53      0.51    291113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# form embedding matrix (w2v)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# padding for word embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec_twitter_50.model\")\n",
    "\n",
    "train_df = pd.read_pickle(\"./train_df_clean.pkl\")\n",
    "test_df = pd.read_pickle(\"./test_df_clean.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "wordEmbedding_w2v_train  = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "vector_dim = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, vector_dim))\n",
    "missingWord = []\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    try:\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        missingWord.append(word)\n",
    "        \n",
    "#modeling\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "div = int(train_df.shape[0]*0.8)\n",
    "\n",
    "wordEmbedding_w2v_X_train = wordEmbedding_w2v_train[:div]\n",
    "wordEmbedding_w2v_y_train = train_df['emotion'][:div]\n",
    "\n",
    "wordEmbedding_w2v_X_test = wordEmbedding_w2v_train[div:]\n",
    "wordEmbedding_w2v_y_test = train_df['emotion'][div:]\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "def encode(y_train, y_test):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y_train)\n",
    "    print('check label: ', label_encoder.classes_)\n",
    "    print('\\n## Before convert')\n",
    "    print('y_train[0:4]:\\n', y_train[0:4])\n",
    "    print('\\ny_train.shape: ', y_train.shape)\n",
    "    print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "    re_y_train = label_encode(label_encoder, y_train)\n",
    "    re_y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "    print('\\n\\n## After convert')\n",
    "    print('y_train[0:4]:\\n', y_train[0:4])\n",
    "    print('\\ny_train.shape: ', y_train.shape)\n",
    "    print('y_test.shape: ', y_test.shape)\n",
    "    \n",
    "    return re_y_train, re_y_test, label_encoder\n",
    "\n",
    "le_wordEmbedding_w2v_y_train, le_wordEmbedding_w2v_y_test, label_encoder = encode(wordEmbedding_w2v_y_train, wordEmbedding_w2v_y_test)\n",
    "\n",
    "# I/O check\n",
    "input_shape = wordEmbedding_w2v_X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Dropout, Activation, ActivityRegularization, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import Constant\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            vector_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "# model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))#0.2\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "# adam = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(\n",
    "    optimizer=\"nadam\", \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "history = model.fit(wordEmbedding_w2v_X_train, le_wordEmbedding_w2v_y_train, \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                    validation_data = (wordEmbedding_w2v_X_test, le_wordEmbedding_w2v_y_test))\n",
    "\n",
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_result = model.predict(x = wordEmbedding_w2v_X_test, batch_size=128)\n",
    "y_pred_result = label_decode(label_encoder, y_pred_result)\n",
    "print(classification_report(y_true=wordEmbedding_w2v_y_test, y_pred=y_pred_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411972,)\n"
     ]
    }
   ],
   "source": [
    "## predict\n",
    "# tests = pd.read_pickle(\"test_df_clean.pkl\")\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"clean_text\"])\n",
    "test_sequences  = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred_result = model.predict(test_sequences, batch_size=128)\n",
    "pred_result = np.array(label_decode(label_encoder, pred_result))\n",
    "print(pred_result.shape)\n",
    "test_df['emotion'] = pred_result\n",
    "test_df = test_df.drop('hashtags', axis=1)\n",
    "test_df = test_df.drop('text', axis=1)\n",
    "test_df = test_df.drop('identification', axis=1)\n",
    "test_df = test_df.drop('clean_text', axis=1)\n",
    "test_df = test_df.drop('tokenized', axis=1)\n",
    "test_df.rename(columns={'tweet_id':'id'}, inplace=True)\n",
    "\n",
    "test_df.to_csv('prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "datamining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
